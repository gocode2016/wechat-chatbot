{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(\"dataset/atis-2.train.w-intent.iob\", \"r\").readlines()\n",
    "test_data = open(\"dataset/atis-2.dev.w-intent.iob\", \"r\").readlines()\n",
    "train_data_ed = data_pipeline(train_data)\n",
    "test_data_ed = data_pipeline(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index, index2word, slot2index, index2slot, intent2index, index2intent = \\\n",
    "        get_info_from_training_data(train_data_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = to_index(train_data_ed, word2index, slot2index, intent2index)\n",
    "index_test = to_index(test_data_ed, word2index, slot2index, intent2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_train[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_steps = 50\n",
    "embedding_size = 64\n",
    "hidden_size = 100\n",
    "n_layers = 2\n",
    "batch_size = 16\n",
    "vocab_size = 871\n",
    "slot_size = 122\n",
    "intent_size = 22\n",
    "epoch_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(tf.int32, [input_steps, batch_size],\n",
    "                                             name='encoder_inputs')\n",
    "# 每句输入的实际长度，除了padding\n",
    "encoder_inputs_actual_length = tf.placeholder(tf.int32, [batch_size],\n",
    "                                                   name='encoder_inputs_actual_length')\n",
    "decoder_targets = tf.placeholder(tf.int32, [batch_size, input_steps],\n",
    "                                      name='decoder_targets')\n",
    "intent_targets = tf.placeholder(tf.int32, [batch_size],\n",
    "                                     name='intent_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size],\n",
    "                                                        -0.1, 0.1), dtype=tf.float32, name=\"embedding\")\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_f_cell = LSTMCell(hidden_size)\n",
    "encoder_b_cell = LSTMCell(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(encoder_fw_outputs, encoder_bw_outputs), (encoder_fw_final_state, encoder_bw_final_state) = \\\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_f_cell,\n",
    "                                    cell_bw=encoder_b_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_actual_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_W = tf.Variable(tf.random_uniform([hidden_size * 2, slot_size], -1, 1),\n",
    "                             dtype=tf.float32, name=\"slot_W\")\n",
    "slot_b = tf.Variable(tf.zeros([slot_size]), dtype=tf.float32, name=\"slot_b\")\n",
    "intent_W = tf.Variable(tf.random_uniform([hidden_size * 2, intent_size], -0.1, 0.1),\n",
    "                               dtype=tf.float32, name=\"intent_W\")\n",
    "intent_b = tf.Variable(tf.zeros([intent_size]), dtype=tf.float32, name=\"intent_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_logits = tf.add(tf.matmul(encoder_final_state_h, intent_W), intent_b)\n",
    "intent = tf.argmax(intent_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='SOS') * 2\n",
    "sos_step_embedded = tf.nn.embedding_lookup(embeddings, sos_time_slice)\n",
    "pad_step_embedded = tf.zeros([batch_size, hidden_size * 2 + embedding_size],\n",
    "                             dtype=tf.float32)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
